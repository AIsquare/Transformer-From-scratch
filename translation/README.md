## Transformer translation project From Scratch

- Run the script in colab, with [TRANSLATION NOTEBOOK](https://github.com/AIsquare/Transformer-From-scratch/tree/main/translation).
- For convenience I have exploited the architecture so that it could run in colab if you have compute you can run it in local.
- Though I din't ran it for lot of epochs, the models hasn't spit out anything good.
- But surely on positional encoding it has learned something.

 > pip install -r requirements.txt

```

â”œâ”€â”€ ğŸ“ Models
â”‚ 
â”œâ”€â”€ ğŸ“„ config.py
â”œâ”€â”€ ğŸ“„ dataset.py
â”œâ”€â”€ ğŸ“„ main.py
â”œâ”€â”€ ğŸ“„ temp.py
â”œâ”€â”€ ğŸ“„ tokenizer_en.json
â”œâ”€â”€ ğŸ“„ tokenizer_it.json
â”œâ”€â”€ ğŸ“„ train.py
â”œâ”€â”€ ğŸ“„transformer_tranlation.ipynb
â”œâ”€â”€ ğŸ“„requirements.txt

```
### Encoder Positional encoding
![encoder Positional encoding](enc.png)
### Decoder Positional encoding
![decoder Positional encoding](dec.png)
### Encoder-decoder positional encoding
![encoder decoder Positional encoding](enc-dec.png)

